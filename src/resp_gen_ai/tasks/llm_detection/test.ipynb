{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity & Burstiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 加载预训练模型\n",
    "def load_model(model_name=\"gpt2\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "# 计算困惑度\n",
    "def compute_perplexity(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs.input_ids)\n",
    "    loss = outputs.loss.item()\n",
    "    return np.exp(loss)\n",
    "\n",
    "# 计算突发性\n",
    "def compute_burstiness(sentences):\n",
    "    sentence_lengths = [len(sentence.split()) for sentence in sentences]\n",
    "    mean_length = np.mean(sentence_lengths)\n",
    "    std_length = np.std(sentence_lengths)\n",
    "    return std_length / mean_length  # 标准差与均值比\n",
    "\n",
    "# GPTZero 检测\n",
    "def gptzero_detect(text, tokenizer, model):\n",
    "    sentences = text.split(\". \")  # 简单句子分割\n",
    "    perplexities = [compute_perplexity(sentence, tokenizer, model) for sentence in sentences]\n",
    "    avg_perplexity = np.mean(perplexities)\n",
    "    burstiness = compute_burstiness(sentences)\n",
    "\n",
    "    # 简单阈值判断\n",
    "    is_generated = avg_perplexity < 30 and burstiness < 0.5\n",
    "    return {\"avg_perplexity\": avg_perplexity, \"burstiness\": burstiness, \"is_generated\": is_generated}\n",
    "\n",
    "# 示例运行\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer, model = load_model(\"gpt2\")\n",
    "    text = \"This is a GPT-generated sentence. It is very uniform and lacks diversity.\"\n",
    "    result = gptzero_detect(text, tokenizer, model)\n",
    "    print(f\"Detection Result: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curvature of log probability function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 加载预训练语言模型\n",
    "def load_model(model_name=\"gpt2\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "# 计算对数概率曲率\n",
    "def compute_log_probability_curvature(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        logits = outputs.logits\n",
    "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "        probabilities = torch.exp(log_probs)\n",
    "        curvature = probabilities.var().item()  # 用方差近似曲率\n",
    "    return curvature\n",
    "\n",
    "# 生成扰动文本\n",
    "def perturb_text(text, num_perturbations=10):\n",
    "    words = text.split()\n",
    "    perturbed_texts = []\n",
    "    for _ in range(num_perturbations):\n",
    "        if len(words) > 1:\n",
    "            idx = torch.randint(0, len(words), (1,)).item()\n",
    "            words[idx] = \"[MASK]\"  # 简单替换模拟扰动\n",
    "        perturbed_texts.append(\" \".join(words))\n",
    "    return perturbed_texts\n",
    "\n",
    "# DetectGPT 检测\n",
    "def detect_gpt(text, tokenizer, model, num_perturbations=10, threshold=0.05):\n",
    "    original_curvature = compute_log_probability_curvature(text, tokenizer, model)\n",
    "    perturbed_texts = perturb_text(text, num_perturbations)\n",
    "    perturbed_curvatures = [compute_log_probability_curvature(t, tokenizer, model) for t in perturbed_texts]\n",
    "    \n",
    "    # 计算曲率变化\n",
    "    avg_perturbed_curvature = sum(perturbed_curvatures) / len(perturbed_curvatures)\n",
    "    delta_curvature = abs(avg_perturbed_curvature - original_curvature)\n",
    "    \n",
    "    # 根据阈值判断是否生成\n",
    "    is_generated = delta_curvature < threshold\n",
    "    return {\n",
    "        \"original_curvature\": original_curvature,\n",
    "        \"avg_perturbed_curvature\": avg_perturbed_curvature,\n",
    "        \"delta_curvature\": delta_curvature,\n",
    "        \"is_generated\": is_generated\n",
    "    }\n",
    "\n",
    "# 示例运行\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer, model = load_model(\"gpt2\")\n",
    "    text = \"This is a GPT-generated sentence.\"\n",
    "    result = detect_gpt(text, tokenizer, model)\n",
    "    print(f\"Detection Result: {result}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cold-attack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
